## [Strang Lecture 14: Orthogonal Vectors and Subspaces](https://www.youtube.com/watch?v=YzZUIYRCE38)
### Gilbert Strang

* The rowspace and nullspace are orthogonal (the angle between them is 90
  degrees).  Same for the columnspace and the left nullspace.

* Orthogonal - in N-dimensional space, the angle between vectors is 90 degrees.

* Test for orthogonality - two vectors are orthogonal if the dot product
  ($x^Ty$) is zero.

* Shows the connection between the Pythagorean theorem and orthogonality.
  * Pythagorean theorem: $|x|^2 + |y|^2 = |x+y|^2$
  * Squared length of vector $x$: $x^Tx$
  * When vectors are orthogonal (sub into Pythagorean):

$$

x^Tx + y^Ty = (x+y)^T(x+y)

x^Tx + y^Ty = x^Tx + y^Ty + x^Ty + y^Tx

0 = 2x^Ty

0 = x^Ty

$$

* Zero vector is orthogonal to all vectors.

* Subspace $S$ is orthogonal to subspace $T$ when every vector in $S$ is
  orthogonal to every vector in $T$.

* Rowspace is orthogonal to the nullspace.  Why?
  * $Ax = 0$ defines the nullspace
  * Alternatively, you can think of it as:

$$
    \begin{bmatrix}
        \begin{array}{c}
            \text{row 1 of A} \\
            \text{row 2 of A} \\
            ... \\
            \text{row m of A} \\
        \end{array}
    \end{bmatrix} *
    \begin{bmatrix}
        \begin{array}{r}
            x_1  \\
            ...  \\
            x_n  \\
        \end{array}
    \end{bmatrix}  =  \begin{bmatrix} 0 \\ 0 \\ ... \\ 0 \end{bmatrix}
$$

* Each row of $A$ is orthogonal to $x$ because that row multiplied by $x$
  equals 0.
  * You also have to show that $x$ is orthogonal to every linear combination of
    the rows of $A$.
  * If $c_1\text{row}_1^Tx = 0$ and $c_2\text{row}_2^T = 0$ then use
    distributive property to show that:

$$

(c_1\text{row}_1 + c_2\text{row}_2)^T = 0_{ }

$$

* The rowspace and nullspace carve $R^n$ into two orthogonal subspaces.  The
  columnspace and left nullspace do the same for $R^m$.  They are **orthogonal
  complements** (the complements contain all the vectors in the space they
  carve up).
  * The nullspace contains **all** vectors perpendicular to the row space.

* Up next: solve $Ax=b$ when there is no solutions.
* Consider $A^TA$ (where $A$ is $m \times n$):
  * It's square $n \times n$
  * It's symmetric: $(A^TA)^T = A^TA^{TT} = A^TA$

* The "good" equation used for solving $Ax=b$ when there is no solution is
  achieved by multiplying both sides by $A^T$ to get $A^TAx=A^Tb$.

* The nullspace of $A^TA$ equals the nullspace of $A$.
* The rank of $A^TA$ equals the rank of $A$.
* $A^TA$ is invertible exactly if $A$ has independent columns.

## [Strang Lecture 10: The Four Fundamental Subspaces](https://www.youtube.com/watch?v=nHlE7EgJFds)
### Gilbert Strang

* 4 fundamental subspaces (assume $A$ is $m \times n$):
  * Columnspace: $C(A)$ is in $R^m$
  * Nullspace: $N(A)$ is in $R^n$
  * Rowspace: all combinations the rows of $A$ (i.e. $C(A^T)$) is in $R^n$
  * Left Nullspace: $N(A^T)$ is in $R^m$

* Dimension of the subspaces:
  * Columnspace: $rank(A)$
  * Nullspace: $n - rank(A)$
  * Rowspace: $rank(A)$
  * Left Nullspace: $m - rank(A)$

* Note the sum of the dimensions of the nullspace and rowspace give $n$ (and
  they are both in $R^n$) and the sum of the dimensions of the columnspace and
  left nullspace give $m$ (and they are both in $R^m$).

* How to produce a basis for each subspace:
  * Columnspace: row reduction, use the original vectors that correspond to
    the pivot columns.
  * Nullspace: set each free variable to 1 (and others to zero) to find basis
    vectors (i.e. find the special solutions).
  * Rowspace: the pivot rows directly after getting $A$ into rref.
  * Left Nullspace: row reduce $A^T$ and find the special solutions.

## [Strang Lecture 9: Independence, Basis and Dimension](https://www.youtube.com/watch?v=yjBerM5jWsc)
### Gilbert Strang

* Suppose $A$ is an $m \times n$ matrix with $m < n$ (more unknowns than
  equations).  Then there are nonzero solutions to $Ax=0$ (there is something
  besides the zero vector in the nullspace of $A$).  There will be at least one
  free variable.

* Vectors $x_1, x_2, x_3, x_n$ are **independent** if the only linear
  combination of those vectors that give the zero vector is all zeros.

* If the zero vector is in the set, then the set is always dependent.

* 3 vectors in 2D space are dependent because there will be free variables.

* When $v_1, ..., v_n$ are the columns of $A$, they are independent if the
  nullspace of $A$ is only the zero vector.  They are dependent if $Ac=0$ for
  some nonzero $c$.  In the independent case, the rank of $A$ is $n$. In the
  dependent case, the rank of $A$ is less than $n$.

* The columns of matrix $A$ ($v_1, ..., v_n$) **span** the columnspace of $A$.
  That is, the space consists of all combinations of those vectors.  They are
  not necessarily independent.  However, we're generally interested in a set of
  vectors that both span and space and are independent.  A spanning,
  independent set (a **basis**) is "just right": you need all the vectors to
  generate the space and none are redundant.

* Example: a basis for $R^3$ is $(1,0,0), (0,1,0), (0,0,1)$.

* Asking if the vectors composing a matrix are a basis is the same as asking if
  the matrix is invertible.

* A basis (e.g. for $R^3$) is not unique.  All basis for a space will have the
  same number of vectors.  This number is the **dimension** of the space.

* The (original) pivot columns are the columns you need for a basis of a space.

* The rank of $A$ (the number of pivot columns) is the dimension of the
  columnspace.

* $dim(A) = r$ and $dim(N(A)) = n - r$ for $m \times n$ matrix.





## [Strang Lecture 8: Solving Ax = b: Row Reduced Form R](https://www.youtube.com/watch?v=9Q1q7s1jTzU)
### Gilbert Strang

* We'll reuse our example matrix to explore $Ax=b$:

$$
    \begin{bmatrix}
        \begin{array}{rrrr}
            1 & 2 & 2 &  2 \\
            2 & 4 & 6 &  8 \\
            3 & 6 & 8 & 10 \\
        \end{array}
    \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}
$$

* Let's use an augmented matrix to deal with the right hand side and do
  elimination:

$$
    \begin{bmatrix}
        \begin{array}{rrrrr}
            1 & 2 & 2 &  2 & b_1 \\
            2 & 4 & 6 &  8 & b_2 \\
            3 & 6 & 8 & 10 & b_3 \\
        \end{array}
    \end{bmatrix}
$$

* End up with, the condition for a solution is $0 = -b_1 - b_2 + b_3$
  (e.g. $b = (1, 5, 6)$):

$$
    \begin{bmatrix}
        \begin{array}{rrrrr}
            1 & 2 & 2 & 2 &   b_1             \\
            0 & 0 & 2 & 4 & -2b_1 + b_2       \\
            0 & 0 & 0 & 0 &  -b_1 - b_2 + b_3 \\
        \end{array}
    \end{bmatrix}
$$

* Solvability: a condition on $b$.  $Ax=b$ is solvable when $b$ is in the
  columnspace of $A$, $C(A)$. Alternatively, if a combination of the rows of
  $A$ give the zero row, then the same combination of the components of $b$
  have to give a zero.

* To find complete solution to $Ax=b$:

  * Find a particular solution. One way to find a particular solution: set all
    free variables to zero and then solve for pivot variables.

  $$
    x_p = \begin{bmatrix}
          \begin{array}{r}
                       -2  \\
                        0  \\
               \frac{3}{2} \\
                        0  \\
          \end{array}
          \end{bmatrix}
  $$

  * Add in the nullspace $x_n$ (aka special solutions).

* The complete solution to $Ax=b$ is $ x_{complete} = x_p + x_n $

$$

x_{complete} =
    \begin{bmatrix}
    \begin{array}{r}
              -2  \\
               0  \\
      \frac{3}{2} \\
               0  \\
    \end{array}
    \end{bmatrix}  +
    c_1 *
    \begin{bmatrix}
    \begin{array}{r}
      -2 \\
       1 \\
       0 \\
       0 \\
    \end{array}
    \end{bmatrix}  +
    c_2 *
    \begin{bmatrix}
    \begin{array}{r}
        2 \\
        0 \\
       -2 \\
        1 \\
    \end{array}
    \end{bmatrix}
$$

* Note $ A * (x_p + x_n) = A * x_p + A * x_n = A * x_p + 0 = A * x_p = b$.

* $ x_c $ in this case is a subspace (the nullspace) shifted by the
  $x_p$.  Instead of going through zero, the subspace goes through
  $x_p$.

* Relations between rank $r$ in a $m \times n$ matrix:

  * $ r \leq m$
  * $ r \leq n$

* Full column rank $r = n$: no free variables, nullspace is only the zero
  vector, one or zero solutions to $Ax=b$.  Matrix will look tall and thin.
  Each zero row will be an additional condition on $b$.

* Full row rank $r = m$: One or many solutions for every $b$ (depending if
  there are free variables.  There will be $n-r = n-m$ free variables). Matrix
  will be short and fat.  $m > n$ results in free variables.

* Full row and column rank $r = m = n$.  Square matrix and invertible, one
  solution for every b. Row reduced echelon form is the identity matrix.

* If $r < m$ and $r < n$, then you will have 0 solutions (if the conditions of
  the zero row are not satisifed) or infinite solutions if the conditions of
  the zero row are satisfied.


## [Strang Lecture 7: Solving Ax = 0: Pivot Variables, Special Solutions](https://www.youtube.com/watch?v=VqP2tREMvt0)
### Gilbert Strang

* Use elimination to solve a $3 \times 4$ rectangular matrix ($A$):

$$
    \begin{bmatrix}
        \begin{array}{rrrr}
            1 & 2 & 2 &  2 \\
            2 & 4 & 6 &  8 \\
            3 & 6 & 8 & 10 \\
        \end{array}
    \end{bmatrix}
$$

* During elimination:
    * Nullspace does not change
    * Solutions do not change
    * Columnspace **IS** changing
    * Rowspace does not change (not explicitly mentioned, but assume so)

* Result of elimination ($U$):

$$
    \begin{bmatrix}
        \begin{array}{rrrr}
            1 & 2 & 2 & 2 \\
            0 & 0 & 2 & 4 \\
            0 & 0 & 0 & 0 \\
        \end{array}
    \end{bmatrix}
$$


* No pivot in second column means it's free (a combo of other columns)

* 2 total pivots; they are 1 at (1,1) and 2 at (2,3)

* The result of elimination is in upper echelon form ($U$)

* Rank of matrix == pivots == 2 in this example.

* You can solve $Ux=0$, same solutions as $Ax=0$

* 2 pivot columns, 2 free columns

* You can assign any number to the free columns and then solve the equations
  for the pivots columns.

* Assign $x_2 = 1$ and $x_4 = 0$ to the free variables and solve for $x_1$ and
  $x_3$.  Note $x_1=-2$ and $x_3=0$ after back subbing.  This is a vector in
  the nullspace and any multiple of it is in the nullspace.

* You have two free variables, so you need another vector in the nullspace.
  Now Assign $x_2 = 0$ and $x_4 = 1$ to the free variables and solve for $x_1$
  and $x_3$.  Note $x_1=2$ and $x_3=-2$ after back subbing.  This is anonter
  vector in the nullspace and any multiple of it is in the nullspace.

* Any linear combination of those two vectors are in the nullspace.  You'll get
  one vector in the nullspace for each free column.


$$
nullspace(A) =
    c_1 * \begin{bmatrix}
            \begin{array}{r}
                -2 \\
                 1 \\
                 0 \\
                 0 \\
            \end{array}
          \end{bmatrix}
    +
    c_2 * \begin{bmatrix}
            \begin{array}{r}
                 2 \\
                 0 \\
                -2 \\
                 1 \\
            \end{array}
          \end{bmatrix}
$$

* Rank is equal to the pivot variable count.  Free variables is $n-R$ for an
  $m \times n$ matrix.

* Finding the nullspace: Do elimination.  Set each free variable to one (and
  others to zero) and solve for a vector in the nullspace.

* Matrix $R$ is the reduced row echelon form (rref).  Use the pivots to clean
  up the rows above them and make pivots equal to 1. $R$ for our above example
  is:

$$
    \begin{bmatrix}
        \begin{array}{rrrr}
            1 & 2 & 0 & -2 \\
            0 & 0 & 1 & 2 \\
            0 & 0 & 0 & 0 \\
        \end{array}
    \end{bmatrix}
$$

* Note the identity matrix $I$ is mixed into the rref matrix $R$.

* Typical rref looks like (I is identity matrix, F is free variables, the
  columns from I and F may be intermixed, $r$ pivot rows AND columns, $m-r$
  free rows, $n-r$ free columns):

$$
    \begin{bmatrix}
        \begin{array}{rr}
            I & F \\
            0 & 0 \\
        \end{array}
    \end{bmatrix}
$$

* Nullspace matrix ($N$) - columns are the special solutions. $RN=0$.

$$
    N = \begin{bmatrix}
            \begin{array}{rr}
                -F \\
                 I \\
            \end{array}
        \end{bmatrix}
$$

* I find this part of the lecture confusing.  He is composing matrices ($F$ and
  $I$) that don't quite correspond to the example.

* Rank of $A^T$ is the same as $A$.  $N(A^T)$ is dimension 1 in our example.

## [A tutorial on the free-energy framework for modelling perception and learning](https://www.sciencedirect.com/science/article/pii/S0022249615000759#bbr000050)
### Rafal Bogacz

* The predictive coding model provides a biologically-plausible account of how
  organisms infer stimuli from noisy inputs.  Introduced by Rao and Ballard in
  1999.  Friston extended the model in 2005 to also learn uncertainty
  associated with different features (e.g. attentional mechanism).  Friston's
  model can also be viewed as approximate Bayesian inference based on a
  minimization of free energy.

* This paper provides a tutorial that aims to be broadly accessible to a
  technical audience.

* Conditions for a model to be biological plausible:

    * Local computation - each neuron need only know about its inputs and
      outputs.

    * Local plasticity - localized changes can be used to train the model.

* We start with a problem: a single organism is trying to infer the diameter of
  a food item on the basis of observed light intensity from one noisy light
  receptor.  There exists a non-linear function `g` relating average light
  intensity with size.

* Interesting to note: once distributions aren't standard (e.g. normal or
  otherwise well known) you can't represent them compactly with summary
  statistics.  It seems likely there has to be some sort of approximation going
  on in the brain.

* Also interesting: calculating the bayesian normalization term (the
  denominator) seems difficult for neural systems (also, not super
  straightforward for computer systems, either).

* Suggests that instead of find the whole posterior, we just find the most
  likely size of the food item given the sensor reading.  This is claimed to be
  much more plausible to implement in neural circuits.

* Importantly, the value that maximizes the likelihood (phi) does not depend on
  the denominator so we can not consider it.  Take the natural log of the
  numerator `p(u | phi) * p(phi)` to get `ln(p(u | phi)) + ln(p(phi))`.


## [The free-energy principle: a unified brain theory?](https://www.fil.ion.ucl.ac.uk/~karl/The%20free-energy%20principle%20A%20unified%20brain%20theory.pdf)
### Karl Friston

* Biological systems aim to maintain homeostasis (i.e. they want to be in a
  limited set of physiological and sensory states).  Entropy is defined as the
  average surprise of outcomes sampled from a distribution.  Low entropy means
  that the outcome of a sampling is relatively predictable.  A biological
  system maintaining homeostasis is relatively low entropy in that there are a
  few states you will often be in and many states you will rarely be in.

* Biological agents aim to minimize the long-term average of surprise.

* The long-term imperative of maintaining states within physiological bounds
  translates into minimizing short-term surprise (note: seems like a greedy
  approach).

* Surprise is not just minimized in the state itself, but also in the movement
  between states.  This ends up resulting in states tending toward global
  random attractors (i.e. stable states that "self-correct" small random
  perturbations).

* Free energy is an upper bound on surprise.  While an agent can't directly
  minimize surprise, it can minimize free energy as free energy is a function
  on sensory states and recognition density.  Recognition density is a
  probabilistic representation of what causes a particular sensation.

* Agents can suppress free energy by acting on the world (change sensory input)
  and changing their internal state (change perception).

* Free energy minimization requires agents to have a generative model of the
  world.

* Discussion of Bayesian brain hypothesis: 1) hierarchy is important because it
  allows establishment of priors and 2) these priors are physically encoded in
  the brain (likely using a form of sufficient statistics e.g. the mean and
  stddev for a normal distribution).

* Bayesian brain ultimately views the brain as an inference engine that
  attempts to optimize probabilistic representations of what causes sensory
  input.  This view of the functioning of the brain can be derived from a
  free-energy approach.

* Stopped: principle of efficient coding section (p5)

* **Discussion**: surprise as -log(p).  entropy is the expected surpise.  For most
  processes, correctly modeling minimizes entropy but surprise will likely be
  irreducible.

