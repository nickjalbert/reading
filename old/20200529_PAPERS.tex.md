## [Strang Lecture 14: Orthogonal Vectors and Subspaces](https://www.youtube.com/watch?v=YzZUIYRCE38)
### Gilbert Strang

* The rowspace and nullspace are orthogonal (the angle between them is 90
  degrees).  Same for the columnspace and the left nullspace.

* Orthogonal - in N-dimensional space, the angle between vectors is 90 degrees.

* Test for orthogonality - two vectors are orthogonal if the dot product
  ($x^Ty$) is zero.

* Shows the connection between the Pythagorean theorem and orthogonality.
  * Pythagorean theorem: $|x|^2 + |y|^2 = |x+y|^2$
  * Squared length of vector $x$: $x^Tx$
  * When vectors are orthogonal (sub into Pythagorean):

$$

x^Tx + y^Ty = (x+y)^T(x+y)

x^Tx + y^Ty = x^Tx + y^Ty + x^Ty + y^Tx

0 = 2x^Ty

0 = x^Ty

$$

* Zero vector is orthogonal to all vectors.

* Subspace $S$ is orthogonal to subspace $T$ when every vector in $S$ is
  orthogonal to every vector in $T$.

* Rowspace is orthogonal to the nullspace.  Why?
  * $Ax = 0$ defines the nullspace
  * Alternatively, you can think of it as:

$$
    \begin{bmatrix}
        \begin{array}{c}
            \text{row 1 of A} \\
            \text{row 2 of A} \\
            ... \\
            \text{row m of A} \\
        \end{array}
    \end{bmatrix} *
    \begin{bmatrix}
        \begin{array}{r}
            x_1  \\
            ...  \\
            x_n  \\
        \end{array}
    \end{bmatrix}  =  \begin{bmatrix} 0 \\ 0 \\ ... \\ 0 \end{bmatrix}
$$

* Each row of $A$ is orthogonal to $x$ because that row multiplied by $x$
  equals 0.
  * You also have to show that $x$ is orthogonal to every linear combination of
    the rows of $A$.
  * If $c_1\text{row}_1^Tx = 0$ and $c_2\text{row}_2^T = 0$ then use
    distributive property to show that:

$$

(c_1\text{row}_1 + c_2\text{row}_2)^T = 0_{ }

$$

* The rowspace and nullspace carve $R^n$ into two orthogonal subspaces.  The
  columnspace and left nullspace do the same for $R^m$.  They are **orthogonal
  complements** (the complements contain all the vectors in the space they
  carve up).
  * The nullspace contains **all** vectors perpendicular to the row space.

* Up next: solve $Ax=b$ when there is no solutions.
* Consider $A^TA$ (where $A$ is $m \times n$):
  * It's square $n \times n$
  * It's symmetric: $(A^TA)^T = A^TA^{TT} = A^TA$

* The "good" equation used for solving $Ax=b$ when there is no solution is
  achieved by multiplying both sides by $A^T$ to get $A^TAx=A^Tb$.

* The nullspace of $A^TA$ equals the nullspace of $A$.
* The rank of $A^TA$ equals the rank of $A$.
* $A^TA$ is invertible exactly if $A$ has independent columns.

## [Strang Lecture 10: The Four Fundamental Subspaces](https://www.youtube.com/watch?v=nHlE7EgJFds)
### Gilbert Strang

* 4 fundamental subspaces (assume $A$ is $m \times n$):
  * Columnspace: $C(A)$ is in $R^m$
  * Nullspace: $N(A)$ is in $R^n$
  * Rowspace: all combinations the rows of $A$ (i.e. $C(A^T)$) is in $R^n$
  * Left Nullspace: $N(A^T)$ is in $R^m$

* Dimension of the subspaces:
  * Columnspace: $rank(A)$
  * Nullspace: $n - rank(A)$
  * Rowspace: $rank(A)$
  * Left Nullspace: $m - rank(A)$

* Note the sum of the dimensions of the nullspace and rowspace give $n$ (and
  they are both in $R^n$) and the sum of the dimensions of the columnspace and
  left nullspace give $m$ (and they are both in $R^m$).

* How to produce a basis for each subspace:
  * Columnspace: row reduction, use the original vectors that correspond to
    the pivot columns.
  * Nullspace: set each free variable to 1 (and others to zero) to find basis
    vectors (i.e. find the special solutions).
  * Rowspace: the pivot rows directly after getting $A$ into rref.
  * Left Nullspace: row reduce $A^T$ and find the special solutions.

## [Strang Lecture 9: Independence, Basis and Dimension](https://www.youtube.com/watch?v=yjBerM5jWsc)
### Gilbert Strang

* Suppose $A$ is an $m \times n$ matrix with $m < n$ (more unknowns than
  equations).  Then there are nonzero solutions to $Ax=0$ (there is something
  besides the zero vector in the nullspace of $A$).  There will be at least one
  free variable.

* Vectors $x_1, x_2, x_3, x_n$ are **independent** if the only linear
  combination of those vectors that give the zero vector is all zeros.

* If the zero vector is in the set, then the set is always dependent.

* 3 vectors in 2D space are dependent because there will be free variables.

* When $v_1, ..., v_n$ are the columns of $A$, they are independent if the
  nullspace of $A$ is only the zero vector.  They are dependent if $Ac=0$ for
  some nonzero $c$.  In the independent case, the rank of $A$ is $n$. In the
  dependent case, the rank of $A$ is less than $n$.

* The columns of matrix $A$ ($v_1, ..., v_n$) **span** the columnspace of $A$.
  That is, the space consists of all combinations of those vectors.  They are
  not necessarily independent.  However, we're generally interested in a set of
  vectors that both span and space and are independent.  A spanning,
  independent set (a **basis**) is "just right": you need all the vectors to
  generate the space and none are redundant.

* Example: a basis for $R^3$ is $(1,0,0), (0,1,0), (0,0,1)$.

* Asking if the vectors composing a matrix are a basis is the same as asking if
  the matrix is invertible.

* A basis (e.g. for $R^3$) is not unique.  All basis for a space will have the
  same number of vectors.  This number is the **dimension** of the space.

* The (original) pivot columns are the columns you need for a basis of a space.

* The rank of $A$ (the number of pivot columns) is the dimension of the
  columnspace.

* $dim(A) = r$ and $dim(N(A)) = n - r$ for $m \times n$ matrix.





## [Strang Lecture 8: Solving Ax = b: Row Reduced Form R](https://www.youtube.com/watch?v=9Q1q7s1jTzU)
### Gilbert Strang

* We'll reuse our example matrix to explore $Ax=b$:

$$
    \begin{bmatrix}
        \begin{array}{rrrr}
            1 & 2 & 2 &  2 \\
            2 & 4 & 6 &  8 \\
            3 & 6 & 8 & 10 \\
        \end{array}
    \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}
$$

* Let's use an augmented matrix to deal with the right hand side and do
  elimination:

$$
    \begin{bmatrix}
        \begin{array}{rrrrr}
            1 & 2 & 2 &  2 & b_1 \\
            2 & 4 & 6 &  8 & b_2 \\
            3 & 6 & 8 & 10 & b_3 \\
        \end{array}
    \end{bmatrix}
$$

* End up with, the condition for a solution is $0 = -b_1 - b_2 + b_3$
  (e.g. $b = (1, 5, 6)$):

$$
    \begin{bmatrix}
        \begin{array}{rrrrr}
            1 & 2 & 2 & 2 &   b_1             \\
            0 & 0 & 2 & 4 & -2b_1 + b_2       \\
            0 & 0 & 0 & 0 &  -b_1 - b_2 + b_3 \\
        \end{array}
    \end{bmatrix}
$$

* Solvability: a condition on $b$.  $Ax=b$ is solvable when $b$ is in the
  columnspace of $A$, $C(A)$. Alternatively, if a combination of the rows of
  $A$ give the zero row, then the same combination of the components of $b$
  have to give a zero.

* To find complete solution to $Ax=b$:

  * Find a particular solution. One way to find a particular solution: set all
    free variables to zero and then solve for pivot variables.

  $$
    x_p = \begin{bmatrix}
          \begin{array}{r}
                       -2  \\
                        0  \\
               \frac{3}{2} \\
                        0  \\
          \end{array}
          \end{bmatrix}
  $$

  * Add in the nullspace $x_n$ (aka special solutions).

* The complete solution to $Ax=b$ is $ x_{complete} = x_p + x_n $

$$

x_{complete} =
    \begin{bmatrix}
    \begin{array}{r}
              -2  \\
               0  \\
      \frac{3}{2} \\
               0  \\
    \end{array}
    \end{bmatrix}  +
    c_1 *
    \begin{bmatrix}
    \begin{array}{r}
      -2 \\
       1 \\
       0 \\
       0 \\
    \end{array}
    \end{bmatrix}  +
    c_2 *
    \begin{bmatrix}
    \begin{array}{r}
        2 \\
        0 \\
       -2 \\
        1 \\
    \end{array}
    \end{bmatrix}
$$

* Note $ A * (x_p + x_n) = A * x_p + A * x_n = A * x_p + 0 = A * x_p = b$.

* $ x_c $ in this case is a subspace (the nullspace) shifted by the
  $x_p$.  Instead of going through zero, the subspace goes through
  $x_p$.

* Relations between rank $r$ in a $m \times n$ matrix:

  * $ r \leq m$
  * $ r \leq n$

* Full column rank $r = n$: no free variables, nullspace is only the zero
  vector, one or zero solutions to $Ax=b$.  Matrix will look tall and thin.
  Each zero row will be an additional condition on $b$.

* Full row rank $r = m$: One or many solutions for every $b$ (depending if
  there are free variables.  There will be $n-r = n-m$ free variables). Matrix
  will be short and fat.  $m > n$ results in free variables.

* Full row and column rank $r = m = n$.  Square matrix and invertible, one
  solution for every b. Row reduced echelon form is the identity matrix.

* If $r < m$ and $r < n$, then you will have 0 solutions (if the conditions of
  the zero row are not satisifed) or infinite solutions if the conditions of
  the zero row are satisfied.


## [Strang Lecture 7: Solving Ax = 0: Pivot Variables, Special Solutions](https://www.youtube.com/watch?v=VqP2tREMvt0)
### Gilbert Strang

* Use elimination to solve a $3 \times 4$ rectangular matrix ($A$):

$$
    \begin{bmatrix}
        \begin{array}{rrrr}
            1 & 2 & 2 &  2 \\
            2 & 4 & 6 &  8 \\
            3 & 6 & 8 & 10 \\
        \end{array}
    \end{bmatrix}
$$

* During elimination:
    * Nullspace does not change
    * Solutions do not change
    * Columnspace **IS** changing
    * Rowspace does not change (not explicitly mentioned, but assume so)

* Result of elimination ($U$):

$$
    \begin{bmatrix}
        \begin{array}{rrrr}
            1 & 2 & 2 & 2 \\
            0 & 0 & 2 & 4 \\
            0 & 0 & 0 & 0 \\
        \end{array}
    \end{bmatrix}
$$


* No pivot in second column means it's free (a combo of other columns)

* 2 total pivots; they are 1 at (1,1) and 2 at (2,3)

* The result of elimination is in upper echelon form ($U$)

* Rank of matrix == pivots == 2 in this example.

* You can solve $Ux=0$, same solutions as $Ax=0$

* 2 pivot columns, 2 free columns

* You can assign any number to the free columns and then solve the equations
  for the pivots columns.

* Assign $x_2 = 1$ and $x_4 = 0$ to the free variables and solve for $x_1$ and
  $x_3$.  Note $x_1=-2$ and $x_3=0$ after back subbing.  This is a vector in
  the nullspace and any multiple of it is in the nullspace.

* You have two free variables, so you need another vector in the nullspace.
  Now Assign $x_2 = 0$ and $x_4 = 1$ to the free variables and solve for $x_1$
  and $x_3$.  Note $x_1=2$ and $x_3=-2$ after back subbing.  This is anonter
  vector in the nullspace and any multiple of it is in the nullspace.

* Any linear combination of those two vectors are in the nullspace.  You'll get
  one vector in the nullspace for each free column.


$$
nullspace(A) =
    c_1 * \begin{bmatrix}
            \begin{array}{r}
                -2 \\
                 1 \\
                 0 \\
                 0 \\
            \end{array}
          \end{bmatrix}
    +
    c_2 * \begin{bmatrix}
            \begin{array}{r}
                 2 \\
                 0 \\
                -2 \\
                 1 \\
            \end{array}
          \end{bmatrix}
$$

* Rank is equal to the pivot variable count.  Free variables is $n-R$ for an
  $m \times n$ matrix.

* Finding the nullspace: Do elimination.  Set each free variable to one (and
  others to zero) and solve for a vector in the nullspace.

* Matrix $R$ is the reduced row echelon form (rref).  Use the pivots to clean
  up the rows above them and make pivots equal to 1. $R$ for our above example
  is:

$$
    \begin{bmatrix}
        \begin{array}{rrrr}
            1 & 2 & 0 & -2 \\
            0 & 0 & 1 & 2 \\
            0 & 0 & 0 & 0 \\
        \end{array}
    \end{bmatrix}
$$

* Note the identity matrix $I$ is mixed into the rref matrix $R$.

* Typical rref looks like (I is identity matrix, F is free variables, the
  columns from I and F may be intermixed, $r$ pivot rows AND columns, $m-r$
  free rows, $n-r$ free columns):

$$
    \begin{bmatrix}
        \begin{array}{rr}
            I & F \\
            0 & 0 \\
        \end{array}
    \end{bmatrix}
$$

* Nullspace matrix ($N$) - columns are the special solutions. $RN=0$.

$$
    N = \begin{bmatrix}
            \begin{array}{rr}
                -F \\
                 I \\
            \end{array}
        \end{bmatrix}
$$

* I find this part of the lecture confusing.  He is composing matrices ($F$ and
  $I$) that don't quite correspond to the example.

* Rank of $A^T$ is the same as $A$.  $N(A^T)$ is dimension 1 in our example.

## [A tutorial on the free-energy framework for modelling perception and learning](https://www.sciencedirect.com/science/article/pii/S0022249615000759#bbr000050)
### Rafal Bogacz

* The predictive coding model provides a biologically-plausible account of how
  organisms infer stimuli from noisy inputs.  Introduced by Rao and Ballard in
  1999.  Friston extended the model in 2005 to also learn uncertainty
  associated with different features (e.g. attentional mechanism).  Friston's
  model can also be viewed as approximate Bayesian inference based on a
  minimization of free energy.

* This paper provides a tutorial that aims to be broadly accessible to a
  technical audience.

* Conditions for a model to be biological plausible:

    * Local computation - each neuron need only know about its inputs and
      outputs.

    * Local plasticity - localized changes can be used to train the model.

* We start with a problem: a single organism is trying to infer the diameter of
  a food item on the basis of observed light intensity from one noisy light
  receptor.  There exists a non-linear function `g` relating average light
  intensity with size.

* Interesting to note: once distributions aren't standard (e.g. normal or
  otherwise well known) you can't represent them compactly with summary
  statistics.  It seems likely there has to be some sort of approximation going
  on in the brain.

* Also interesting: calculating the bayesian normalization term (the
  denominator) seems difficult for neural systems (also, not super
  straightforward for computer systems, either).

* Suggests that instead of find the whole posterior, we just find the most
  likely size of the food item given the sensor reading.  This is claimed to be
  much more plausible to implement in neural circuits.

* Importantly, the value that maximizes the likelihood (phi) does not depend on
  the denominator so we can not consider it.  Take the natural log of the
  numerator `p(u | phi) * p(phi)` to get `ln(p(u | phi)) + ln(p(phi))`.


## [The free-energy principle: a unified brain theory?](https://www.fil.ion.ucl.ac.uk/~karl/The%20free-energy%20principle%20A%20unified%20brain%20theory.pdf)
### Karl Friston

* Biological systems aim to maintain homeostasis (i.e. they want to be in a
  limited set of physiological and sensory states).  Entropy is defined as the
  average surprise of outcomes sampled from a distribution.  Low entropy means
  that the outcome of a sampling is relatively predictable.  A biological
  system maintaining homeostasis is relatively low entropy in that there are a
  few states you will often be in and many states you will rarely be in.

* Biological agents aim to minimize the long-term average of surprise.

* The long-term imperative of maintaining states within physiological bounds
  translates into minimizing short-term surprise (note: seems like a greedy
  approach).

* Surprise is not just minimized in the state itself, but also in the movement
  between states.  This ends up resulting in states tending toward global
  random attractors (i.e. stable states that "self-correct" small random
  perturbations).

* Free energy is an upper bound on surprise.  While an agent can't directly
  minimize surprise, it can minimize free energy as free energy is a function
  on sensory states and recognition density.  Recognition density is a
  probabilistic representation of what causes a particular sensation.

* Agents can suppress free energy by acting on the world (change sensory input)
  and changing their internal state (change perception).

* Free energy minimization requires agents to have a generative model of the
  world.

* Discussion of Bayesian brain hypothesis: 1) hierarchy is important because it
  allows establishment of priors and 2) these priors are physically encoded in
  the brain (likely using a form of sufficient statistics e.g. the mean and
  stddev for a normal distribution).

* Bayesian brain ultimately views the brain as an inference engine that
  attempts to optimize probabilistic representations of what causes sensory
  input.  This view of the functioning of the brain can be derived from a
  free-energy approach.

* Stopped: principle of efficient coding section (p5)

* **Discussion**: surprise as -log(p).  entropy is the expected surpise.  For most
  processes, correctly modeling minimizes entropy but surprise will likely be
  irreducible.


## [Lecture 6: Column Space and Nullspace](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/ax-b-and-the-four-subspaces/column-space-and-nullspace)
## Gilbert Strang

* Discussion of subspaces plane P through zero and line L through zero in
  R^3.  A bit confusing the exact definition of union and intersection, but
  if union is not a subspace in general (assuming L is not coplanar with P)
  while intersection is.

* Interestingly:  overdetermined equations (in this example, 3 unknowns and 4
  equations) don't fill the R^4 space so they can't always be solved.
  Overdetermined doesn't fill the subspace in this case.

* Column space :  all `b`'s that solve `Ax=b`.  In an `m`x`n` matrix, column
  space is a subspace R^m

* Null space: all `x`'s that solve `Ax=0`. In an `m`x`n` matrix, null space is
  a subspace of R^n

* Note that the solutions to `Ax=b` do not form a subspace in general for `b`
  (generally will not include the zero vector).  The nullspace is a vector
  space.

* Previewing the idea that you will have a particular solution to Ax=b, but
  then you can add a vector from the nullspace to also get another solution.

## [Surfing Uncertainty, Chapter 3: The Imaginarium](https://global.oup.com/academic/product/surfing-uncertainty-9780190217013)
### Andy Clark

* Evidence is presented that even early entrances to the brain (the V1 system)
  are deeply influenced by the constructive predictions coming top down (not
  just the bottom up senses).

* The VWFA area of the brain is activated during regular reading in normal
  subjects and braille reading in blind subjects.  Multimodal areas like this
  make sense in a hierarchical brain where top-down predictions drive
  perception.

* Our responses to missing stimuli (e.g. a song missing a particular beat) also
  suggest the use of generative models in the brain.  We habituate to sensory
  input, but then the removal of the input triggers a surprise response.

* Gives an example of listening to a familiar song through a bad radio.  You
  know the song but you can also focus attention and hear the bad quality of
  the radio (turn up the gain on the prediction error).

* Evidence that perception can occur faster to well-predicted stimuli.  Once
  the top-down model is in accord with the sensory input, perception occurs
  (and this happens faster with well-predicted stimuli).

* The generative nature of our top-down models can be repurposed for
  imagination (endogenous generation of sensory-like states). Perception
  co-emerges and is a dual of imagination.

* Reddy et al. experiment where brain signals were recorded as subjects viewed
  an image and as subjects imagined an image.  Classifiers were trained on both
  sets of data.  Classifiers were successful at picking out the images.
  Classifiers could also be swapped (i.e. the viewing classifier worked for the
  imagining data).  This suggests that the same pathways are activating by
  viewing and imagining.

* Evidence is presented that neurotransmitter balance determines how heavily we
  weight prediction error and thus how much of our modeling is constrained by
  sensory input vs in a hallucinating/dream state.

* Argument that sleep is used to regularize/prune overly complex and overfit
  generative models.

* PIMMS (predictive interactive multi-memory system): episodic (recalling
  specific times and place), semantic (knowing what something is), perceptual
  (recalling specific sensory percepts).


## [Introduction to Bayesian data analysis - Part 3: How to do Bayes?](https://www.youtube.com/watch?v=Ie-6H_r7I5A)
### Rasmus Bååth

* How to actual use Bayes in practice:  Approximate Bayesian computation (as
  we did in part 1 and 2) is slow.  Faster models all require that the
  generative model allows you to directly calculate the probability of seeing
  a particular result.  Faster models also explore the parameter space in a
  smarter way.

* relates Bayes to maximum likelihood estimation

* I find the model at time 5:30 a bit confusing.  My guess is that `y` is
  sampled from a normal distribution where the mean of the distribution is
  determined by `intercept + slope * x` and the stddev is held constant.  So,
  to generate a y, you select an x, plug it into the line equation to get the
  mean, and then sample for the normal with the determined mean (and chosen
  stddev).  Presumably, the sampling of the normal represents noise in the
  process.

* "Now this is a generative model, but it is not yet a Bayesian model.  For
  that, we need to represent all uncertainty by probability, and add prior
  distributions over all parameters.

* Result of Bayesian linear regression gives you an idea of how likely the
  various parameters are to have generated the data.

* TODO: find another explanation of a Bayesian approach to linear regression.

* Markov Chain Monte Carlo (MCMC) allows for exploration of complicated parameter
  spaces.  They walk around the parameter space and sample from the probability
  distributions.  They will revisit and sample each parameter set in proportion
  to how likely it is (to have generated the data).

* Stan is a language for Bayesian modelling.  You define your model in Stan and
  then it takes cares of fitting it.

* Things that can go wrong in MCMC: initial parameter values are way off, our
  algorithm doesn't have enough time to explore the parameter space, algorithm
  gets stuck at a local maximum.  Very similar to optimization.


## [Introduction to Bayesian data analysis - Part 2: Why use Bayes?](https://www.youtube.com/watch?v=mAUwjSo5TJE)
### Rasmus Bååth

This video illustrates how you might take a Bayesian approach to A/B testing.
The setup:

* We have our brochure experiment (method A) from before where 6 out of 16
  people signed up.
* We have a new experiment (brochure + sample, method B) where 10 out of 16
  people signed up.

We run the same algorithm from before to generate a posterior for method B.
Interestingly, you have to run the experiments in parallel and only keep the
results if both match (i.e. you draw a **p** from both priors, run the
simulation, and only keep the draws if both matched the observed data).  You
end up with a posterior distribution for both method A and method B.

What you also end up with is a set of rows where each row has two columns: the
chosen **p** for method A and the chosen **p** for method B that generated the
observed data. You can then add another processing step where you take the
difference between the two columns to get a third distribution and reason
about how likely it is that the true parameter for A is greater (or less than)
the true parameter for B.

Note, you can incorporate other information (e.g. expert opinion) in the
priors.  In the video, they model this using a beta distribution.  The
posterior ends up looking like a mix of the prior and the data.  The more data
you have, the more the posterior ends up looking like the data.

Bayesian analysis also retains the uncertainty around the estimated parameters
which can be useful for decision making. (Question: what if we're uncertain
about the generative model?).


## [Introduction to Bayesian data analysis - Part 1: What is Bayes?](https://www.youtube.com/watch?v=3OJEae7Qb_o)
### Rasmus Bååth

Key insight here is you can think of Bayes as requiring:

* Some data
* A generative model
* Priors

The motivating example was a fish-of-the-month club.  You run an experiment
where 6 out of 16 people (data) sign up after receiving a brochure.  You want
to get an estimate of the true sign-up rate.  You model the sign up choice as
a binomial with some unknown **p** (generative model) and you don't have a strong
thought on what the actual sign up rate is (uniform prior between 0% and
100%).

We will generate our posterior "guess" of the actual **p** as follows
(Approximate Bayesian Computation):

* Pick a **p** from the prior (uniform so p=.1 and p=.9 are equally likely to
  be selected)

* Plug the selected **p** into a binomial model, simulate the binomial with
  n=16 and p=**p**

* If the proportion of "successful" flips in the simulation equals the 6 we
  observed in the data, count the trial as a success.

* Repeat thousands of times.

What you end up with is a posterior distribution of the **p**'s most likely to
generate the 6 out of 16 you saw.  This looks normal around 37.5%.  This is
Bayes in a nutshell.  You take your prior, gather some evidence, and update
your prior into the posterior to incorporate your evidence.

**THOUGHTS**: very useful video series to generate some Bayesian intuition.
