## [Strang Lecture 14: Orthogonal Vectors and Subspaces](https://www.youtube.com/watch?v=YzZUIYRCE38)
### Gilbert Strang

* The rowspace and nullspace are orthogonal (the angle between them is 90
  degrees).  Same for the columnspace and the left nullspace.

* Orthogonal - in N-dimensional space, the angle between vectors is 90 degrees.

* Test for orthogonality - two vectors are orthogonal if the dot product
  (<img src="/old/tex/835f8d843b2e8684eb2db4c5330cbfd1.svg?invert_in_darkmode&sanitize=true" align=middle width=28.399804949999993pt height=27.6567522pt/>) is zero.

* Shows the connection between the Pythagorean theorem and orthogonality.
  * Pythagorean theorem: <img src="/old/tex/0020492d10a7da15fd6f957fe4bae301.svg?invert_in_darkmode&sanitize=true" align=middle width=146.88721244999996pt height=26.76175259999998pt/>
  * Squared length of vector <img src="/old/tex/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode&sanitize=true" align=middle width=9.39498779999999pt height=14.15524440000002pt/>: <img src="/old/tex/71e959d58d2486c3eb2942a9177b20f6.svg?invert_in_darkmode&sanitize=true" align=middle width=29.14556864999999pt height=27.6567522pt/>
  * When vectors are orthogonal (sub into Pythagorean):

<p align="center"><img src="/old/tex/40c0cc53edc6552d83b24e1e7988d9cb.svg?invert_in_darkmode&sanitize=true" align=middle width=271.9619727pt height=76.202808pt/></p>

* Zero vector is orthogonal to all vectors.

* Subspace <img src="/old/tex/e257acd1ccbe7fcb654708f1a866bfe9.svg?invert_in_darkmode&sanitize=true" align=middle width=11.027402099999989pt height=22.465723500000017pt/> is orthogonal to subspace <img src="/old/tex/2f118ee06d05f3c2d98361d9c30e38ce.svg?invert_in_darkmode&sanitize=true" align=middle width=11.889314249999991pt height=22.465723500000017pt/> when every vector in <img src="/old/tex/e257acd1ccbe7fcb654708f1a866bfe9.svg?invert_in_darkmode&sanitize=true" align=middle width=11.027402099999989pt height=22.465723500000017pt/> is
  orthogonal to every vector in <img src="/old/tex/2f118ee06d05f3c2d98361d9c30e38ce.svg?invert_in_darkmode&sanitize=true" align=middle width=11.889314249999991pt height=22.465723500000017pt/>.

* Rowspace is orthogonal to the nullspace.  Why?
  * <img src="/old/tex/1ee1e21dada7761855899433bd89f236.svg?invert_in_darkmode&sanitize=true" align=middle width=51.86062694999999pt height=22.465723500000017pt/> defines the nullspace
  * Alternatively, you can think of it as:

<p align="center"><img src="/old/tex/e9a21ea3768b166d4ca1923b94bf3890.svg?invert_in_darkmode&sanitize=true" align=middle width=249.89546174999998pt height=78.9048876pt/></p>

* Each row of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> is orthogonal to <img src="/old/tex/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode&sanitize=true" align=middle width=9.39498779999999pt height=14.15524440000002pt/> because that row multiplied by <img src="/old/tex/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode&sanitize=true" align=middle width=9.39498779999999pt height=14.15524440000002pt/>
  equals 0.
  * You also have to show that <img src="/old/tex/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode&sanitize=true" align=middle width=9.39498779999999pt height=14.15524440000002pt/> is orthogonal to every linear combination of
    the rows of $A$.
  * If <img src="/old/tex/d3640f47b400f5f568d7920beea97886.svg?invert_in_darkmode&sanitize=true" align=middle width=90.44877434999998pt height=27.6567522pt/> and <img src="/old/tex/a387aee3908b9bf388a76f7295e4e7e1.svg?invert_in_darkmode&sanitize=true" align=middle width=81.05378654999998pt height=27.6567522pt/> then use
    distributive property to show that:

<p align="center"><img src="/old/tex/857fb6816ad65754e5f97d20f91875e9.svg?invert_in_darkmode&sanitize=true" align=middle width=169.43283555pt height=17.9379651pt/></p>

* The rowspace and nullspace carve <img src="/old/tex/73915ecf85c52fbc3bf42267f60059e4.svg?invert_in_darkmode&sanitize=true" align=middle width=20.73449399999999pt height=22.465723500000017pt/> into two orthogonal subspaces.  The
  columnspace and left nullspace do the same for <img src="/old/tex/10bbb5859fbc71ff8d85cbeac2b323ef.svg?invert_in_darkmode&sanitize=true" align=middle width=24.27331994999999pt height=22.465723500000017pt/>.  They are **orthogonal
  complements** (the complements contain all the vectors in the space they
  carve up).
  * The nullspace contains **all** vectors perpendicular to the row space.

* Up next: solve <img src="/old/tex/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode&sanitize=true" align=middle width=50.69621369999999pt height=22.831056599999986pt/> when there is no solutions.
* Consider <img src="/old/tex/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode&sanitize=true" align=middle width=35.01318974999999pt height=27.6567522pt/> (where <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> is <img src="/old/tex/205995f88b807b2f5268f7ef4053f049.svg?invert_in_darkmode&sanitize=true" align=middle width=44.39116769999999pt height=19.1781018pt/>):
  * It's square <img src="/old/tex/3add1221abfa79cb14021bc2dacd5725.svg?invert_in_darkmode&sanitize=true" align=middle width=39.82494449999999pt height=19.1781018pt/>
  * It's symmetric: <img src="/old/tex/68ac6c189cb6071cd258e8d987eefd41.svg?invert_in_darkmode&sanitize=true" align=middle width=191.90512274999998pt height=27.6567522pt/>

* The "good" equation used for solving <img src="/old/tex/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode&sanitize=true" align=middle width=50.69621369999999pt height=22.831056599999986pt/> when there is no solution is
  achieved by multiplying both sides by <img src="/old/tex/99f7812af37ee7004df7177a1e821ec5.svg?invert_in_darkmode&sanitize=true" align=middle width=21.86251649999999pt height=27.6567522pt/> to get <img src="/old/tex/1210dd4ad39a067584505367e08fb1ae.svg?invert_in_darkmode&sanitize=true" align=middle width=96.06499484999999pt height=27.6567522pt/>.

* The nullspace of <img src="/old/tex/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode&sanitize=true" align=middle width=35.01318974999999pt height=27.6567522pt/> equals the nullspace of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/>.
* The rank of <img src="/old/tex/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode&sanitize=true" align=middle width=35.01318974999999pt height=27.6567522pt/> equals the rank of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/>.
* <img src="/old/tex/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode&sanitize=true" align=middle width=35.01318974999999pt height=27.6567522pt/> is invertible exactly if <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> has independent columns.

## [Strang Lecture 10: The Four Fundamental Subspaces](https://www.youtube.com/watch?v=nHlE7EgJFds)
### Gilbert Strang

* 4 fundamental subspaces (assume <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> is <img src="/old/tex/205995f88b807b2f5268f7ef4053f049.svg?invert_in_darkmode&sanitize=true" align=middle width=44.39116769999999pt height=19.1781018pt/>):
  * Columnspace: <img src="/old/tex/34e5f1f13b2812eee1b9520c4affa446.svg?invert_in_darkmode&sanitize=true" align=middle width=38.038868999999984pt height=24.65753399999998pt/> is in <img src="/old/tex/10bbb5859fbc71ff8d85cbeac2b323ef.svg?invert_in_darkmode&sanitize=true" align=middle width=24.27331994999999pt height=22.465723500000017pt/>
  * Nullspace: <img src="/old/tex/b1254f33d689433f6a0588c2d401f1c9.svg?invert_in_darkmode&sanitize=true" align=middle width=40.11420104999999pt height=24.65753399999998pt/> is in <img src="/old/tex/73915ecf85c52fbc3bf42267f60059e4.svg?invert_in_darkmode&sanitize=true" align=middle width=20.73449399999999pt height=22.465723500000017pt/>
  * Rowspace: all combinations the rows of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> (i.e. <img src="/old/tex/295c58fc21b40e5fc782651c6e6d3af2.svg?invert_in_darkmode&sanitize=true" align=middle width=48.39446204999999pt height=27.6567522pt/>) is in <img src="/old/tex/73915ecf85c52fbc3bf42267f60059e4.svg?invert_in_darkmode&sanitize=true" align=middle width=20.73449399999999pt height=22.465723500000017pt/>
  * Left Nullspace: <img src="/old/tex/f14f60eb1a566660335ef95423c1d84f.svg?invert_in_darkmode&sanitize=true" align=middle width=50.469792449999986pt height=27.6567522pt/> is in <img src="/old/tex/10bbb5859fbc71ff8d85cbeac2b323ef.svg?invert_in_darkmode&sanitize=true" align=middle width=24.27331994999999pt height=22.465723500000017pt/>

* Dimension of the subspaces:
  * Columnspace: <img src="/old/tex/cdc45231ecb69a0453183503bf5d0f94.svg?invert_in_darkmode&sanitize=true" align=middle width=60.61857944999999pt height=24.65753399999998pt/>
  * Nullspace: <img src="/old/tex/72df1852be8011a748586479c025c3f1.svg?invert_in_darkmode&sanitize=true" align=middle width=90.5766477pt height=24.65753399999998pt/>
  * Rowspace: <img src="/old/tex/cdc45231ecb69a0453183503bf5d0f94.svg?invert_in_darkmode&sanitize=true" align=middle width=60.61857944999999pt height=24.65753399999998pt/>
  * Left Nullspace: <img src="/old/tex/9c2d2e8db322d12b62be0f80789bde97.svg?invert_in_darkmode&sanitize=true" align=middle width=95.14287089999998pt height=24.65753399999998pt/>

* Note the sum of the dimensions of the nullspace and rowspace give <img src="/old/tex/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode&sanitize=true" align=middle width=9.86687624999999pt height=14.15524440000002pt/> (and
  they are both in <img src="/old/tex/73915ecf85c52fbc3bf42267f60059e4.svg?invert_in_darkmode&sanitize=true" align=middle width=20.73449399999999pt height=22.465723500000017pt/>) and the sum of the dimensions of the columnspace and
  left nullspace give <img src="/old/tex/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode&sanitize=true" align=middle width=14.433101099999991pt height=14.15524440000002pt/> (and they are both in <img src="/old/tex/10bbb5859fbc71ff8d85cbeac2b323ef.svg?invert_in_darkmode&sanitize=true" align=middle width=24.27331994999999pt height=22.465723500000017pt/>).

* How to produce a basis for each subspace:
  * Columnspace: row reduction, use the original vectors that correspond to
    the pivot columns.
  * Nullspace: set each free variable to 1 (and others to zero) to find basis
    vectors (i.e. find the special solutions).
  * Rowspace: the pivot rows directly after getting <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> into rref.
  * Left Nullspace: row reduce <img src="/old/tex/99f7812af37ee7004df7177a1e821ec5.svg?invert_in_darkmode&sanitize=true" align=middle width=21.86251649999999pt height=27.6567522pt/> and find the special solutions.

## [Strang Lecture 9: Independence, Basis and Dimension](https://www.youtube.com/watch?v=yjBerM5jWsc)
### Gilbert Strang

* Suppose <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> is an <img src="/old/tex/205995f88b807b2f5268f7ef4053f049.svg?invert_in_darkmode&sanitize=true" align=middle width=44.39116769999999pt height=19.1781018pt/> matrix with <img src="/old/tex/ffef4f323b92402b2f3599acad545fc2.svg?invert_in_darkmode&sanitize=true" align=middle width=46.21760714999999pt height=17.723762100000005pt/> (more unknowns than
  equations).  Then there are nonzero solutions to <img src="/old/tex/2b71965bdc17323260ed22a8cc29538d.svg?invert_in_darkmode&sanitize=true" align=middle width=51.86062694999999pt height=22.465723500000017pt/> (there is something
  besides the zero vector in the nullspace of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/>).  There will be at least one
  free variable.

* Vectors <img src="/old/tex/4a763fc825d9ff2a2c3fc0e796f97fea.svg?invert_in_darkmode&sanitize=true" align=middle width=89.74700459999998pt height=14.15524440000002pt/> are **independent** if the only linear
  combination of those vectors that give the zero vector is all zeros.

* If the zero vector is in the set, then the set is always dependent.

* 3 vectors in 2D space are dependent because there will be free variables.

* When <img src="/old/tex/e5d37a40e17a9f823ccffea6bea3c34a.svg?invert_in_darkmode&sanitize=true" align=middle width=59.74705439999998pt height=14.15524440000002pt/> are the columns of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/>, they are independent if the
  nullspace of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> is only the zero vector.  They are dependent if <img src="/old/tex/afad16e925d155cfae4780fb9906a379.svg?invert_in_darkmode&sanitize=true" align=middle width=49.57944254999999pt height=22.465723500000017pt/> for
  some nonzero <img src="/old/tex/3e18a4a28fdee1744e5e3f79d13b9ff6.svg?invert_in_darkmode&sanitize=true" align=middle width=7.11380504999999pt height=14.15524440000002pt/>.  In the independent case, the rank of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> is <img src="/old/tex/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode&sanitize=true" align=middle width=9.86687624999999pt height=14.15524440000002pt/>. In the
  dependent case, the rank of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> is less than <img src="/old/tex/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode&sanitize=true" align=middle width=9.86687624999999pt height=14.15524440000002pt/>.

* The columns of matrix <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> (<img src="/old/tex/e5d37a40e17a9f823ccffea6bea3c34a.svg?invert_in_darkmode&sanitize=true" align=middle width=59.74705439999998pt height=14.15524440000002pt/>) **span** the columnspace of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/>.
  That is, the space consists of all combinations of those vectors.  They are
  not necessarily independent.  However, we're generally interested in a set of
  vectors that both span and space and are independent.  A spanning,
  independent set (a **basis**) is "just right": you need all the vectors to
  generate the space and none are redundant.

* Example: a basis for <img src="/old/tex/88b8a2e57772e3be969c3fc12c2a7095.svg?invert_in_darkmode&sanitize=true" align=middle width=19.161017699999988pt height=26.76175259999998pt/> is <img src="/old/tex/d788d125bda3a57b95ad6b747fcb754b.svg?invert_in_darkmode&sanitize=true" align=middle width=170.77625069999996pt height=24.65753399999998pt/>.

* Asking if the vectors composing a matrix are a basis is the same as asking if
  the matrix is invertible.

* A basis (e.g. for <img src="/old/tex/88b8a2e57772e3be969c3fc12c2a7095.svg?invert_in_darkmode&sanitize=true" align=middle width=19.161017699999988pt height=26.76175259999998pt/>) is not unique.  All basis for a space will have the
  same number of vectors.  This number is the **dimension** of the space.

* The (original) pivot columns are the columns you need for a basis of a space.

* The rank of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> (the number of pivot columns) is the dimension of the
  columnspace.

* <img src="/old/tex/753f1af03b020fb1f43f81229af029fe.svg?invert_in_darkmode&sanitize=true" align=middle width=83.55710879999998pt height=24.65753399999998pt/> and <img src="/old/tex/06d8f359ad5b10da879f08225b4e68ad.svg?invert_in_darkmode&sanitize=true" align=middle width=141.3005781pt height=24.65753399999998pt/> for <img src="/old/tex/205995f88b807b2f5268f7ef4053f049.svg?invert_in_darkmode&sanitize=true" align=middle width=44.39116769999999pt height=19.1781018pt/> matrix.





## [Strang Lecture 8: Solving Ax = b: Row Reduced Form R](https://www.youtube.com/watch?v=9Q1q7s1jTzU)
### Gilbert Strang

* We'll reuse our example matrix to explore <img src="/old/tex/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode&sanitize=true" align=middle width=50.69621369999999pt height=22.831056599999986pt/>:

<p align="center"><img src="/old/tex/695d6dcfed1fa253519b8dd53016e86e.svg?invert_in_darkmode&sanitize=true" align=middle width=187.03206225pt height=59.1786591pt/></p>

* Let's use an augmented matrix to deal with the right hand side and do
  elimination:

<p align="center"><img src="/old/tex/4545a8ad84c6167498141eb954e12ead.svg?invert_in_darkmode&sanitize=true" align=middle width=159.6349128pt height=59.1786591pt/></p>

* End up with, the condition for a solution is <img src="/old/tex/225d150b596970a35004bd81427462a5.svg?invert_in_darkmode&sanitize=true" align=middle width=125.57051265pt height=22.831056599999986pt/>
  (e.g. <img src="/old/tex/5f5355935f9d9f19d9ddc9f2bf9b91fc.svg?invert_in_darkmode&sanitize=true" align=middle width=81.02725454999998pt height=24.65753399999998pt/>):

<p align="center"><img src="/old/tex/4b4a6ca02aaa1b4fddf64e514525d8e0.svg?invert_in_darkmode&sanitize=true" align=middle width=233.24198205pt height=59.1786591pt/></p>

* Solvability: a condition on <img src="/old/tex/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode&sanitize=true" align=middle width=7.054796099999991pt height=22.831056599999986pt/>.  <img src="/old/tex/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode&sanitize=true" align=middle width=50.69621369999999pt height=22.831056599999986pt/> is solvable when <img src="/old/tex/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode&sanitize=true" align=middle width=7.054796099999991pt height=22.831056599999986pt/> is in the
  columnspace of <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/>, <img src="/old/tex/34e5f1f13b2812eee1b9520c4affa446.svg?invert_in_darkmode&sanitize=true" align=middle width=38.038868999999984pt height=24.65753399999998pt/>. Alternatively, if a combination of the rows of
  <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/> give the zero row, then the same combination of the components of <img src="/old/tex/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode&sanitize=true" align=middle width=7.054796099999991pt height=22.831056599999986pt/>
  have to give a zero.

* To find complete solution to <img src="/old/tex/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode&sanitize=true" align=middle width=50.69621369999999pt height=22.831056599999986pt/>:

  * Find a particular solution. One way to find a particular solution: set all
    free variables to zero and then solve for pivot variables.

  <p align="center"><img src="/old/tex/43d8a479203a8cb80758dbd37bae51e1.svg?invert_in_darkmode&sanitize=true" align=middle width=98.27185829999999pt height=78.9048876pt/></p>

  * Add in the nullspace <img src="/old/tex/d7084ce258ffe96f77e4f3647b250bbf.svg?invert_in_darkmode&sanitize=true" align=middle width=17.521011749999992pt height=14.15524440000002pt/> (aka special solutions).

* The complete solution to <img src="/old/tex/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode&sanitize=true" align=middle width=50.69621369999999pt height=22.831056599999986pt/> is <img src="/old/tex/2bc7a0501b098605f46d03e2abb36260.svg?invert_in_darkmode&sanitize=true" align=middle width=139.20797055pt height=19.1781018pt/>

<p align="center"><img src="/old/tex/276344100dbd931ddd3dea5278002148.svg?invert_in_darkmode&sanitize=true" align=middle width=362.17469475pt height=78.9048876pt/></p>

* Note <img src="/old/tex/ca2029a5cbe6fc73d2db721fec0638df.svg?invert_in_darkmode&sanitize=true" align=middle width=419.93166765pt height=24.65753399999998pt/>.

* <img src="/old/tex/773dc52501a09d3dc40596571a2321dd.svg?invert_in_darkmode&sanitize=true" align=middle width=15.26963954999999pt height=14.15524440000002pt/> in this case is a subspace (the nullspace) shifted by the
  <img src="/old/tex/0957a9bbf01d2ea5f621f1d068d404d9.svg?invert_in_darkmode&sanitize=true" align=middle width=16.17146519999999pt height=14.15524440000002pt/>.  Instead of going through zero, the subspace goes through
  <img src="/old/tex/0957a9bbf01d2ea5f621f1d068d404d9.svg?invert_in_darkmode&sanitize=true" align=middle width=16.17146519999999pt height=14.15524440000002pt/>.

* Relations between rank <img src="/old/tex/89f2e0d2d24bcf44db73aab8fc03252c.svg?invert_in_darkmode&sanitize=true" align=middle width=7.87295519999999pt height=14.15524440000002pt/> in a <img src="/old/tex/205995f88b807b2f5268f7ef4053f049.svg?invert_in_darkmode&sanitize=true" align=middle width=44.39116769999999pt height=19.1781018pt/> matrix:

  * <img src="/old/tex/9dad75ceef24a34ae5a4707a7d1571d5.svg?invert_in_darkmode&sanitize=true" align=middle width=44.22368609999999pt height=20.908638300000003pt/>
  * <img src="/old/tex/e5a277555371ce571a0d3f06d3ad7ed7.svg?invert_in_darkmode&sanitize=true" align=middle width=39.65746289999999pt height=20.908638300000003pt/>

* Full column rank <img src="/old/tex/e6b88fbaec17edf03d3fd1adf98db331.svg?invert_in_darkmode&sanitize=true" align=middle width=39.65746289999999pt height=14.15524440000002pt/>: no free variables, nullspace is only the zero
  vector, one or zero solutions to <img src="/old/tex/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode&sanitize=true" align=middle width=50.69621369999999pt height=22.831056599999986pt/>.  Matrix will look tall and thin.
  Each zero row will be an additional condition on <img src="/old/tex/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode&sanitize=true" align=middle width=7.054796099999991pt height=22.831056599999986pt/>.

* Full row rank <img src="/old/tex/972a93691ff5ceb688f5e875b61f528d.svg?invert_in_darkmode&sanitize=true" align=middle width=44.22368609999999pt height=14.15524440000002pt/>: One or many solutions for every <img src="/old/tex/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode&sanitize=true" align=middle width=7.054796099999991pt height=22.831056599999986pt/> (depending if
  there are free variables.  There will be <img src="/old/tex/48d45f061bd81cbb30840cc309e2b25b.svg?invert_in_darkmode&sanitize=true" align=middle width=104.13982095pt height=19.1781018pt/> free variables). Matrix
  will be short and fat.  <img src="/old/tex/079bfec7814e7f4bcbae5a5e2830bb51.svg?invert_in_darkmode&sanitize=true" align=middle width=46.21760714999999pt height=17.723762100000005pt/> results in free variables.

* Full row and column rank <img src="/old/tex/c6578e573264528b0d6927ee0c7a1176.svg?invert_in_darkmode&sanitize=true" align=middle width=76.00819379999999pt height=14.15524440000002pt/>.  Square matrix and invertible, one
  solution for every b. Row reduced echelon form is the identity matrix.

* If <img src="/old/tex/d65635677d427f79f5afec59fb1e8022.svg?invert_in_darkmode&sanitize=true" align=middle width=44.22368609999999pt height=17.723762100000005pt/> and <img src="/old/tex/797f7e1e586399c71e8b56b04c7af670.svg?invert_in_darkmode&sanitize=true" align=middle width=39.65746289999999pt height=17.723762100000005pt/>, then you will have 0 solutions (if the conditions of
  the zero row are not satisifed) or infinite solutions if the conditions of
  the zero row are satisfied.


## [Strang Lecture 7: Solving Ax = 0: Pivot Variables, Special Solutions](https://www.youtube.com/watch?v=VqP2tREMvt0)
### Gilbert Strang

* Use elimination to solve a <img src="/old/tex/5b4cf7163dd6f95ba26235a3efa57ac2.svg?invert_in_darkmode&sanitize=true" align=middle width=36.52961069999999pt height=21.18721440000001pt/> rectangular matrix (<img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/>):

<p align="center"><img src="/old/tex/bebe32224cc9eb809d4cdde8d74f24ac.svg?invert_in_darkmode&sanitize=true" align=middle width=128.76731835pt height=59.1786591pt/></p>

* During elimination:
    * Nullspace does not change
    * Solutions do not change
    * Columnspace **IS** changing
    * Rowspace does not change (not explicitly mentioned, but assume so)

* Result of elimination (<img src="/old/tex/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode&sanitize=true" align=middle width=13.01596064999999pt height=22.465723500000017pt/>):

<p align="center"><img src="/old/tex/45a25ec04e9bb57f0e505f6908eb87ef.svg?invert_in_darkmode&sanitize=true" align=middle width=120.5481156pt height=59.1786591pt/></p>


* No pivot in second column means it's free (a combo of other columns)

* 2 total pivots; they are 1 at (1,1) and 2 at (2,3)

* The result of elimination is in upper echelon form (<img src="/old/tex/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode&sanitize=true" align=middle width=13.01596064999999pt height=22.465723500000017pt/>)

* Rank of matrix == pivots == 2 in this example.

* You can solve <img src="/old/tex/2fe657088322d47b689cd44cc71ccd05.svg?invert_in_darkmode&sanitize=true" align=middle width=52.54776779999999pt height=22.465723500000017pt/>, same solutions as <img src="/old/tex/2b71965bdc17323260ed22a8cc29538d.svg?invert_in_darkmode&sanitize=true" align=middle width=51.86062694999999pt height=22.465723500000017pt/>

* 2 pivot columns, 2 free columns

* You can assign any number to the free columns and then solve the equations
  for the pivots columns.

* Assign <img src="/old/tex/56139c2ecb22056c1ab58d6e6fc736a1.svg?invert_in_darkmode&sanitize=true" align=middle width=46.90628744999999pt height=21.18721440000001pt/> and <img src="/old/tex/face1f7a70bce3be1526cc862d0b9567.svg?invert_in_darkmode&sanitize=true" align=middle width=46.90628744999999pt height=21.18721440000001pt/> to the free variables and solve for <img src="/old/tex/277fbbae7d4bc65b6aa601ea481bebcc.svg?invert_in_darkmode&sanitize=true" align=middle width=15.94753544999999pt height=14.15524440000002pt/> and
  <img src="/old/tex/2c52641cc5fa73cbbdf887c89d82f0de.svg?invert_in_darkmode&sanitize=true" align=middle width=15.94753544999999pt height=14.15524440000002pt/>.  Note <img src="/old/tex/9ff44075699a748a2691b4dabf6780eb.svg?invert_in_darkmode&sanitize=true" align=middle width=59.69172164999999pt height=21.18721440000001pt/> and <img src="/old/tex/1412bdb3a4c77853bafc5e9e1b7150e5.svg?invert_in_darkmode&sanitize=true" align=middle width=46.90628744999999pt height=21.18721440000001pt/> after back subbing.  This is a vector in
  the nullspace and any multiple of it is in the nullspace.

* You have two free variables, so you need another vector in the nullspace.
  Now Assign <img src="/old/tex/3b556ebecf31f20220e296e4687393a3.svg?invert_in_darkmode&sanitize=true" align=middle width=46.90628744999999pt height=21.18721440000001pt/> and <img src="/old/tex/fb6f7317d69927ab9949587450b52cd8.svg?invert_in_darkmode&sanitize=true" align=middle width=46.90628744999999pt height=21.18721440000001pt/> to the free variables and solve for <img src="/old/tex/277fbbae7d4bc65b6aa601ea481bebcc.svg?invert_in_darkmode&sanitize=true" align=middle width=15.94753544999999pt height=14.15524440000002pt/>
  and <img src="/old/tex/2c52641cc5fa73cbbdf887c89d82f0de.svg?invert_in_darkmode&sanitize=true" align=middle width=15.94753544999999pt height=14.15524440000002pt/>.  Note <img src="/old/tex/bda947926e3a56a7e1f79bf2d7d9c985.svg?invert_in_darkmode&sanitize=true" align=middle width=46.90628744999999pt height=21.18721440000001pt/> and <img src="/old/tex/6815dc5c67a03ae13e30eb0b4a95d2c7.svg?invert_in_darkmode&sanitize=true" align=middle width=59.69172164999999pt height=21.18721440000001pt/> after back subbing.  This is anonter
  vector in the nullspace and any multiple of it is in the nullspace.

* Any linear combination of those two vectors are in the nullspace.  You'll get
  one vector in the nullspace for each free column.


<p align="center"><img src="/old/tex/f49c3fd306ccbf36ab5d041a67f507e3.svg?invert_in_darkmode&sanitize=true" align=middle width=315.03822405pt height=78.9048876pt/></p>

* Rank is equal to the pivot variable count.  Free variables is <img src="/old/tex/f7ac410dd4413138a6d3f58028ae8c58.svg?invert_in_darkmode&sanitize=true" align=middle width=42.566541599999994pt height=22.465723500000017pt/> for an
  <img src="/old/tex/205995f88b807b2f5268f7ef4053f049.svg?invert_in_darkmode&sanitize=true" align=middle width=44.39116769999999pt height=19.1781018pt/> matrix.

* Finding the nullspace: Do elimination.  Set each free variable to one (and
  others to zero) and solve for a vector in the nullspace.

* Matrix <img src="/old/tex/1e438235ef9ec72fc51ac5025516017c.svg?invert_in_darkmode&sanitize=true" align=middle width=12.60847334999999pt height=22.465723500000017pt/> is the reduced row echelon form (rref).  Use the pivots to clean
  up the rows above them and make pivots equal to 1. <img src="/old/tex/1e438235ef9ec72fc51ac5025516017c.svg?invert_in_darkmode&sanitize=true" align=middle width=12.60847334999999pt height=22.465723500000017pt/> for our above example
  is:

<p align="center"><img src="/old/tex/7959c9d92984071c0784524172664d9f.svg?invert_in_darkmode&sanitize=true" align=middle width=133.33354319999998pt height=59.1786591pt/></p>

* Note the identity matrix <img src="/old/tex/21fd4e8eecd6bdf1a4d3d6bd1fb8d733.svg?invert_in_darkmode&sanitize=true" align=middle width=8.515988249999989pt height=22.465723500000017pt/> is mixed into the rref matrix <img src="/old/tex/1e438235ef9ec72fc51ac5025516017c.svg?invert_in_darkmode&sanitize=true" align=middle width=12.60847334999999pt height=22.465723500000017pt/>.

* Typical rref looks like (I is identity matrix, F is free variables, the
  columns from I and F may be intermixed, <img src="/old/tex/89f2e0d2d24bcf44db73aab8fc03252c.svg?invert_in_darkmode&sanitize=true" align=middle width=7.87295519999999pt height=14.15524440000002pt/> pivot rows AND columns, <img src="/old/tex/6705abc37a8daba2b601e9e9771af618.svg?invert_in_darkmode&sanitize=true" align=middle width=42.397246649999985pt height=19.1781018pt/>
  free rows, <img src="/old/tex/bdc4fe24ac8ca702834e1744502f09d9.svg?invert_in_darkmode&sanitize=true" align=middle width=37.83102344999999pt height=19.1781018pt/> free columns):

<p align="center"><img src="/old/tex/10382425eb948366c69148d1b0ecdfac.svg?invert_in_darkmode&sanitize=true" align=middle width=71.5982586pt height=39.452455349999994pt/></p>

* Nullspace matrix (<img src="/old/tex/f9c4988898e7f532b9f826a75014ed3c.svg?invert_in_darkmode&sanitize=true" align=middle width=14.99998994999999pt height=22.465723500000017pt/>) - columns are the special solutions. <img src="/old/tex/942733788955d96b64cf2a3ee7984e69.svg?invert_in_darkmode&sanitize=true" align=middle width=57.74527934999999pt height=22.465723500000017pt/>.

<p align="center"><img src="/old/tex/c3408736ad0e11f3506d57d5defff14d.svg?invert_in_darkmode&sanitize=true" align=middle width=96.3469683pt height=39.452455349999994pt/></p>

* I find this part of the lecture confusing.  He is composing matrices (<img src="/old/tex/b8bc815b5e9d5177af01fd4d3d3c2f10.svg?invert_in_darkmode&sanitize=true" align=middle width=12.85392569999999pt height=22.465723500000017pt/> and
  <img src="/old/tex/21fd4e8eecd6bdf1a4d3d6bd1fb8d733.svg?invert_in_darkmode&sanitize=true" align=middle width=8.515988249999989pt height=22.465723500000017pt/>) that don't quite correspond to the example.

* Rank of <img src="/old/tex/99f7812af37ee7004df7177a1e821ec5.svg?invert_in_darkmode&sanitize=true" align=middle width=21.86251649999999pt height=27.6567522pt/> is the same as <img src="/old/tex/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode&sanitize=true" align=middle width=12.32879834999999pt height=22.465723500000017pt/>.  <img src="/old/tex/f14f60eb1a566660335ef95423c1d84f.svg?invert_in_darkmode&sanitize=true" align=middle width=50.469792449999986pt height=27.6567522pt/> is dimension 1 in our example.

## [A tutorial on the free-energy framework for modelling perception and learning](https://www.sciencedirect.com/science/article/pii/S0022249615000759#bbr000050)
### Rafal Bogacz

* The predictive coding model provides a biologically-plausible account of how
  organisms infer stimuli from noisy inputs.  Introduced by Rao and Ballard in
  1999.  Friston extended the model in 2005 to also learn uncertainty
  associated with different features (e.g. attentional mechanism).  Friston's
  model can also be viewed as approximate Bayesian inference based on a
  minimization of free energy.

* This paper provides a tutorial that aims to be broadly accessible to a
  technical audience.

* Conditions for a model to be biological plausible:

    * Local computation - each neuron need only know about its inputs and
      outputs.

    * Local plasticity - localized changes can be used to train the model.

* We start with a problem: a single organism is trying to infer the diameter of
  a food item on the basis of observed light intensity from one noisy light
  receptor.  There exists a non-linear function `g` relating average light
  intensity with size.

* Interesting to note: once distributions aren't standard (e.g. normal or
  otherwise well known) you can't represent them compactly with summary
  statistics.  It seems likely there has to be some sort of approximation going
  on in the brain.

* Also interesting: calculating the bayesian normalization term (the
  denominator) seems difficult for neural systems (also, not super
  straightforward for computer systems, either).

* Suggests that instead of find the whole posterior, we just find the most
  likely size of the food item given the sensor reading.  This is claimed to be
  much more plausible to implement in neural circuits.

* Importantly, the value that maximizes the likelihood (phi) does not depend on
  the denominator so we can not consider it.  Take the natural log of the
  numerator `p(u | phi) * p(phi)` to get `ln(p(u | phi)) + ln(p(phi))`.


## [The free-energy principle: a unified brain theory?](https://www.fil.ion.ucl.ac.uk/~karl/The%20free-energy%20principle%20A%20unified%20brain%20theory.pdf)
### Karl Friston

* Biological systems aim to maintain homeostasis (i.e. they want to be in a
  limited set of physiological and sensory states).  Entropy is defined as the
  average surprise of outcomes sampled from a distribution.  Low entropy means
  that the outcome of a sampling is relatively predictable.  A biological
  system maintaining homeostasis is relatively low entropy in that there are a
  few states you will often be in and many states you will rarely be in.

* Biological agents aim to minimize the long-term average of surprise.

* The long-term imperative of maintaining states within physiological bounds
  translates into minimizing short-term surprise (note: seems like a greedy
  approach).

* Surprise is not just minimized in the state itself, but also in the movement
  between states.  This ends up resulting in states tending toward global
  random attractors (i.e. stable states that "self-correct" small random
  perturbations).

* Free energy is an upper bound on surprise.  While an agent can't directly
  minimize surprise, it can minimize free energy as free energy is a function
  on sensory states and recognition density.  Recognition density is a
  probabilistic representation of what causes a particular sensation.

* Agents can suppress free energy by acting on the world (change sensory input)
  and changing their internal state (change perception).

* Free energy minimization requires agents to have a generative model of the
  world.

* Discussion of Bayesian brain hypothesis: 1) hierarchy is important because it
  allows establishment of priors and 2) these priors are physically encoded in
  the brain (likely using a form of sufficient statistics e.g. the mean and
  stddev for a normal distribution).

* Bayesian brain ultimately views the brain as an inference engine that
  attempts to optimize probabilistic representations of what causes sensory
  input.  This view of the functioning of the brain can be derived from a
  free-energy approach.

* Stopped: principle of efficient coding section (p5)

* **Discussion**: surprise as -log(p).  entropy is the expected surpise.  For most
  processes, correctly modeling minimizes entropy but surprise will likely be
  irreducible.


## [Lecture 6: Column Space and Nullspace](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/ax-b-and-the-four-subspaces/column-space-and-nullspace)
## Gilbert Strang

* Discussion of subspaces plane P through zero and line L through zero in
  R^3.  A bit confusing the exact definition of union and intersection, but
  if union is not a subspace in general (assuming L is not coplanar with P)
  while intersection is.

* Interestingly:  overdetermined equations (in this example, 3 unknowns and 4
  equations) don't fill the R^4 space so they can't always be solved.
  Overdetermined doesn't fill the subspace in this case.

* Column space :  all `b`'s that solve `Ax=b`.  In an `m`x`n` matrix, column
  space is a subspace R^m

* Null space: all `x`'s that solve `Ax=0`. In an `m`x`n` matrix, null space is
  a subspace of R^n

* Note that the solutions to `Ax=b` do not form a subspace in general for `b`
  (generally will not include the zero vector).  The nullspace is a vector
  space.

* Previewing the idea that you will have a particular solution to Ax=b, but
  then you can add a vector from the nullspace to also get another solution.

## [Surfing Uncertainty, Chapter 3: The Imaginarium](https://global.oup.com/academic/product/surfing-uncertainty-9780190217013)
### Andy Clark

* Evidence is presented that even early entrances to the brain (the V1 system)
  are deeply influenced by the constructive predictions coming top down (not
  just the bottom up senses).

* The VWFA area of the brain is activated during regular reading in normal
  subjects and braille reading in blind subjects.  Multimodal areas like this
  make sense in a hierarchical brain where top-down predictions drive
  perception.

* Our responses to missing stimuli (e.g. a song missing a particular beat) also
  suggest the use of generative models in the brain.  We habituate to sensory
  input, but then the removal of the input triggers a surprise response.

* Gives an example of listening to a familiar song through a bad radio.  You
  know the song but you can also focus attention and hear the bad quality of
  the radio (turn up the gain on the prediction error).

* Evidence that perception can occur faster to well-predicted stimuli.  Once
  the top-down model is in accord with the sensory input, perception occurs
  (and this happens faster with well-predicted stimuli).

* The generative nature of our top-down models can be repurposed for
  imagination (endogenous generation of sensory-like states). Perception
  co-emerges and is a dual of imagination.

* Reddy et al. experiment where brain signals were recorded as subjects viewed
  an image and as subjects imagined an image.  Classifiers were trained on both
  sets of data.  Classifiers were successful at picking out the images.
  Classifiers could also be swapped (i.e. the viewing classifier worked for the
  imagining data).  This suggests that the same pathways are activating by
  viewing and imagining.

* Evidence is presented that neurotransmitter balance determines how heavily we
  weight prediction error and thus how much of our modeling is constrained by
  sensory input vs in a hallucinating/dream state.

* Argument that sleep is used to regularize/prune overly complex and overfit
  generative models.

* PIMMS (predictive interactive multi-memory system): episodic (recalling
  specific times and place), semantic (knowing what something is), perceptual
  (recalling specific sensory percepts).


## [Introduction to Bayesian data analysis - Part 3: How to do Bayes?](https://www.youtube.com/watch?v=Ie-6H_r7I5A)
### Rasmus Bååth

* How to actual use Bayes in practice:  Approximate Bayesian computation (as
  we did in part 1 and 2) is slow.  Faster models all require that the
  generative model allows you to directly calculate the probability of seeing
  a particular result.  Faster models also explore the parameter space in a
  smarter way.

* relates Bayes to maximum likelihood estimation

* I find the model at time 5:30 a bit confusing.  My guess is that `y` is
  sampled from a normal distribution where the mean of the distribution is
  determined by `intercept + slope * x` and the stddev is held constant.  So,
  to generate a y, you select an x, plug it into the line equation to get the
  mean, and then sample for the normal with the determined mean (and chosen
  stddev).  Presumably, the sampling of the normal represents noise in the
  process.

* "Now this is a generative model, but it is not yet a Bayesian model.  For
  that, we need to represent all uncertainty by probability, and add prior
  distributions over all parameters.

* Result of Bayesian linear regression gives you an idea of how likely the
  various parameters are to have generated the data.

* TODO: find another explanation of a Bayesian approach to linear regression.

* Markov Chain Monte Carlo (MCMC) allows for exploration of complicated parameter
  spaces.  They walk around the parameter space and sample from the probability
  distributions.  They will revisit and sample each parameter set in proportion
  to how likely it is (to have generated the data).

* Stan is a language for Bayesian modelling.  You define your model in Stan and
  then it takes cares of fitting it.

* Things that can go wrong in MCMC: initial parameter values are way off, our
  algorithm doesn't have enough time to explore the parameter space, algorithm
  gets stuck at a local maximum.  Very similar to optimization.


## [Introduction to Bayesian data analysis - Part 2: Why use Bayes?](https://www.youtube.com/watch?v=mAUwjSo5TJE)
### Rasmus Bååth

This video illustrates how you might take a Bayesian approach to A/B testing.
The setup:

* We have our brochure experiment (method A) from before where 6 out of 16
  people signed up.
* We have a new experiment (brochure + sample, method B) where 10 out of 16
  people signed up.

We run the same algorithm from before to generate a posterior for method B.
Interestingly, you have to run the experiments in parallel and only keep the
results if both match (i.e. you draw a **p** from both priors, run the
simulation, and only keep the draws if both matched the observed data).  You
end up with a posterior distribution for both method A and method B.

What you also end up with is a set of rows where each row has two columns: the
chosen **p** for method A and the chosen **p** for method B that generated the
observed data. You can then add another processing step where you take the
difference between the two columns to get a third distribution and reason
about how likely it is that the true parameter for A is greater (or less than)
the true parameter for B.

Note, you can incorporate other information (e.g. expert opinion) in the
priors.  In the video, they model this using a beta distribution.  The
posterior ends up looking like a mix of the prior and the data.  The more data
you have, the more the posterior ends up looking like the data.

Bayesian analysis also retains the uncertainty around the estimated parameters
which can be useful for decision making. (Question: what if we're uncertain
about the generative model?).


## [Introduction to Bayesian data analysis - Part 1: What is Bayes?](https://www.youtube.com/watch?v=3OJEae7Qb_o)
### Rasmus Bååth

Key insight here is you can think of Bayes as requiring:

* Some data
* A generative model
* Priors

The motivating example was a fish-of-the-month club.  You run an experiment
where 6 out of 16 people (data) sign up after receiving a brochure.  You want
to get an estimate of the true sign-up rate.  You model the sign up choice as
a binomial with some unknown **p** (generative model) and you don't have a strong
thought on what the actual sign up rate is (uniform prior between 0% and
100%).

We will generate our posterior "guess" of the actual **p** as follows
(Approximate Bayesian Computation):

* Pick a **p** from the prior (uniform so p=.1 and p=.9 are equally likely to
  be selected)

* Plug the selected **p** into a binomial model, simulate the binomial with
  n=16 and p=**p**

* If the proportion of "successful" flips in the simulation equals the 6 we
  observed in the data, count the trial as a success.

* Repeat thousands of times.

What you end up with is a posterior distribution of the **p**'s most likely to
generate the 6 out of 16 you saw.  This looks normal around 37.5%.  This is
Bayes in a nutshell.  You take your prior, gather some evidence, and update
your prior into the posterior to incorporate your evidence.

**THOUGHTS**: very useful video series to generate some Bayesian intuition.
